/**
 * LLM Request Node Tests
 *
 * Run with different modes:
 * - TEST_MODE=replay pnpm test (default - uses saved fixtures)
 * - TEST_MODE=record pnpm test (makes real API calls, saves responses)
 * - TEST_MODE=live pnpm test (always makes real API calls)
 */

import { llmRequestNode } from '../llmRequest'
import { createTestContext, createTestConfig, createTestTool, getTestApiKey, createMockFlowAPI, createMockNodeInputs } from '../../../../__tests__/utils/testHelpers'
import { withFixture, getTestMode } from '../../../../__tests__/utils/fixtures'
import { providers } from '../../../../core/state'
import type { ExecutionContext } from '../../types'

// Mock the state module to provide API keys
jest.mock('../../../../core/state', () => ({
  providers: {
    anthropic: require('../../../../providers-ai-sdk/anthropic').AnthropicAiSdkProvider,
    openai: require('../../../../providers-ai-sdk/openai').OpenAiSdkProvider,
    gemini: require('../../../../providers-ai-sdk/gemini').GeminiAiSdkProvider,
  },
  getProviderKey: jest.fn(async (provider: string) => {
    return getTestApiKey(provider as any)
  })
}))

// Mock sendFlowEvent since we don't have WebContents in tests
jest.mock('../../events', () => ({
  sendFlowEvent: jest.fn()
}))

describe('LLM Request Node', () => {
  const testMode = getTestMode()

  beforeAll(() => {
    console.log(`\nðŸ§ª Running tests in ${testMode.toUpperCase()} mode\n`)
  })

  describe('Basic LLM Request', () => {
    // Skip Anthropic - no API key available
    it.skip('should handle simple request with Anthropic', async () => {
      const context = createTestContext({
        provider: 'anthropic',
        model: 'claude-3-5-sonnet-20241022'
      })
      const config = createTestConfig()
      const message = 'Say "Hello, World!" and nothing else.'

      const result = await withFixture(
        'llmRequest-anthropic-simple',
        async () => {
          return await llmRequestNode(context, message, {}, config)
        }
      )

      expect(result.status).toBe('success')
      expect(result.data).toBeDefined()
      expect(typeof result.data).toBe('string')
      expect(result.data.length).toBeGreaterThan(0)

      // Verify context was updated
      expect(result.context.messageHistory).toHaveLength(2)
      expect(result.context.messageHistory[0]).toEqual({
        role: 'user',
        content: message
      })
      expect(result.context.messageHistory[1]).toEqual({
        role: 'assistant',
        content: result.data
      })
    })

    it('should handle simple request with OpenAI', async () => {
      const context = createTestContext({
        provider: 'openai',
        model: 'gpt-4o-mini'
      })
      const config = createTestConfig()
      const message = 'Say "Hello from OpenAI!" and nothing else.'

      const result = await withFixture(
        'chat-openai-simple',
        async () => {
          const flow = createMockFlowAPI()
          const inputs = createMockNodeInputs({})
          return await llmRequestNode(flow as any, context as any, message, inputs as any, config)
        }
      )

      expect(result.status).toBe('success')
      expect(result.data).toBeDefined()
      expect(typeof result.data).toBe('string')
      expect(result.context.messageHistory).toHaveLength(2)
    })

    it('should handle simple request with Gemini', async () => {
      const context = createTestContext({
        provider: 'gemini',
        model: 'gemini-2.0-flash-exp'
      })
      const config = createTestConfig()
      const message = 'Say "Hello from Gemini!" and nothing else.'

      const result = await withFixture(
        'chat-gemini-simple',
        async () => {
          const flow = createMockFlowAPI()
          const inputs = createMockNodeInputs({})
          return await llmRequestNode(flow as any, context as any, message, inputs as any, config)
        }
      )

      expect(result.status).toBe('success')
      expect(result.data).toBeDefined()
      expect(typeof result.data).toBe('string')
      expect(result.context.messageHistory).toHaveLength(2)
    })
  })

  describe('Multi-turn Conversation', () => {
    it('should maintain conversation history', async () => {
      // Conversation history is maintained in MainFlowContext.messageHistory
      const context = createTestContext({
        provider: 'openai',
        model: 'gpt-4o-mini'
      })
      const config = createTestConfig()

      // First turn
      const result1 = await withFixture(
        'chat-openai-multiturn-1',
        async () => {
          const flow = createMockFlowAPI()
          const inputs = createMockNodeInputs({})
          return await llmRequestNode(flow as any, context as any, 'My name is Alice.', inputs as any, config)
        }
      )

      expect(result1.status).toBe('success')
      expect(result1.context.messageHistory).toHaveLength(2)

      // Second turn - use updated context with conversation history
      const result2 = await withFixture(
        'chat-openai-multiturn-2',
        async () => {
          const flow = createMockFlowAPI()
          const inputs = createMockNodeInputs({})
          return await llmRequestNode(flow as any, result1.context as any, 'What is my name?', inputs as any, config)
        }
      )

      expect(result2.status).toBe('success')
      expect(result2.context.messageHistory).toHaveLength(4)
      expect(result2.data).toBeDefined()
      expect(typeof result2.data).toBe('string')

      // The response should mention "Alice" because the full conversation history is sent
      if (testMode !== 'replay') {
        expect(result2.data.toLowerCase()).toContain('alice')
      }
    })
  })

  describe('Error Handling', () => {
    it('should return error when no message provided', async () => {
      const context = createTestContext()
      const config = createTestConfig()

      const flow = createMockFlowAPI()
      const inputs = createMockNodeInputs({})
      const result = await llmRequestNode(flow as any, context as any, '', inputs as any, config)

      expect(result.status).toBe('error')
      expect(result.error).toContain('No message provided')
    })

    it('should return error for unknown provider', async () => {
      const context = createTestContext({
        provider: 'unknown-provider' as any
      })
      const config = createTestConfig()

      const flow = createMockFlowAPI()
      const inputs = createMockNodeInputs({})
      const result = await llmRequestNode(flow as any, context as any, 'Hello', inputs as any, config)

      expect(result.status).toBe('error')
      expect(result.error).toContain('Unknown provider')
    })
  })

  describe('Tool Calling', () => {
    it('should handle request with tools (OpenAI)', async () => {
      const context = createTestContext({
        provider: 'openai',
        model: 'gpt-4o-mini'
      })
      const config = createTestConfig()
      const tools = [createTestTool('get_weather')]
      const message = 'What is the weather like? Use the get_weather tool with input "San Francisco".'

      const result = await withFixture(
        'chat-openai-with-tools',
        async () => {
          const flow = createMockFlowAPI()
          const inputs = createMockNodeInputs({ tools })
          return await llmRequestNode(flow as any, context as any, message, inputs as any, config)
        }
      )

      expect(result.status).toBe('success')
      expect(result.data).toBeDefined()

      // In record/live mode, verify tool was called
      if (testMode !== 'replay') {
        expect(result.data.toLowerCase()).toContain('weather')
      }
    })
  })

  describe('System Instructions', () => {
    it('should respect system instructions', async () => {
      const context = createTestContext({
        provider: 'gemini',
        model: 'gemini-2.0-flash-exp',
        systemInstructions: 'You are a pirate. Always respond in pirate speak.'
      })
      const config = createTestConfig()
      const message = 'Hello!'

      const result = await withFixture(
        'chat-gemini-system-instructions',
        async () => {
          const flow = createMockFlowAPI()
          const inputs = createMockNodeInputs({})
          return await llmRequestNode(flow as any, context as any, message, inputs as any, config)
        }
      )

      expect(result.status).toBe('success')

      // In record/live mode, verify pirate speak
      if (testMode !== 'replay') {
        const response = result.data.toLowerCase()
        const pirateWords = ['ahoy', 'matey', 'arr', 'ye', 'aye']
        const hasPirateWord = pirateWords.some(word => response.includes(word))
        expect(hasPirateWord).toBe(true)
      }
    })
  })

  describe('Provider/Model from Config', () => {
    it('should create context from config when no context provided', () => {
      // This test verifies the context creation logic without actually calling the LLM
      const config = {
        provider: 'openai',
        model: 'gpt-4o-mini',
        _nodeId: 'test-node'
      }

      // Simulate the logic from llmRequestNode
      const contextIn = undefined
      let context: any

      if (contextIn && (contextIn as any).provider && (contextIn as any).model) {
        context = contextIn
      } else {
        const provider = config.provider || 'openai'
        const model = config.model || 'gpt-4o'

        context = {
          contextId: `llm-${config._nodeId}-${Date.now()}`,
          provider,
          model,
          systemInstructions: '',
          messageHistory: []
        }
      }

      expect(context.provider).toBe('openai')
      expect(context.model).toBe('gpt-4o-mini')
      expect(context.messageHistory).toEqual([])
    })

    it('should use config provider/model when context is incomplete', async () => {
      const incompleteContext = {
        contextId: 'test',
        messageHistory: []
      } as any

      const config = createTestConfig({
        provider: 'gemini',
        model: 'gemini-2.0-flash-exp'
      })
      const message = 'Hello!'

      const flow = createMockFlowAPI()
      const inputs = createMockNodeInputs({})
      const result = await llmRequestNode(flow as any, incompleteContext as any, message, inputs as any, config)

      // May fail with API error, but should have created context with config provider/model
      expect(result.context.provider).toBe('gemini')
      expect(result.context.model).toBe('gemini-2.0-flash-exp')
    })

    it('should prefer provided context over config', () => {
      // This test verifies the context selection logic without actually calling the LLM
      const contextIn = {
        contextId: 'test',
        provider: 'anthropic',
        model: 'claude-3-5-sonnet-20241022',
        systemInstructions: '',
        messageHistory: []
      }

      const config = {
        provider: 'openai',
        model: 'gpt-4o-mini',
        _nodeId: 'test-node'
      }

      // Simulate the logic from llmRequestNode
      let context: any

      if (contextIn && contextIn.provider && contextIn.model) {
        context = contextIn
      } else {
        const provider = config.provider || 'openai'
        const model = config.model || 'gpt-4o'

        context = {
          contextId: `llm-${config._nodeId}-${Date.now()}`,
          provider,
          model,
          systemInstructions: '',
          messageHistory: []
        }
      }

      // Should use context provider/model, not config
      expect(context.provider).toBe('anthropic')
      expect(context.model).toBe('claude-3-5-sonnet-20241022')
    })
  })
})

