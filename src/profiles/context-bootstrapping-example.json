{
  "name": "context-bootstrapping-example",
  "description": "Example flow demonstrating context bootstrapping with injectMessages node. Uses a low-cost LLM to generate codebase summary, then injects it as synthetic conversation history for the main chat.",
  "version": "1.0.0",
  "nodes": [
    {
      "id": "defaultContextStart",
      "kind": "defaultContextStart",
      "config": {
        "systemInstructions": "You are a helpful coding assistant with deep knowledge of this codebase."
      },
      "position": { "x": 100, "y": 100 },
      "expanded": true
    },
    {
      "id": "manualInput-bootstrap",
      "kind": "manualInput",
      "label": "Bootstrap Query",
      "config": {
        "message": "Please provide a concise summary of this codebase's architecture, key components, and main technologies used. Keep it under 500 words."
      },
      "position": { "x": 100, "y": 250 },
      "expanded": true
    },
    {
      "id": "tools-bootstrap",
      "kind": "tools",
      "label": "Bootstrap Tools",
      "config": {
        "tools": [
          "fs.read_file",
          "fs.read_dir",
          "index.search",
          "code.search_ast"
        ]
      },
      "position": { "x": 100, "y": 400 },
      "expanded": false
    },
    {
      "id": "llmRequest-bootstrap",
      "kind": "llmRequest",
      "label": "Generate Summary (gpt-4o-mini)",
      "config": {},
      "position": { "x": 300, "y": 400 },
      "expanded": false
    },
    {
      "id": "injectMessages-bootstrap",
      "kind": "injectMessages",
      "label": "Inject Bootstrap Context",
      "config": {
        "staticUserMessage": "Please provide a concise summary of this codebase's architecture, key components, and main technologies used.",
        "injectionMode": "prepend",
        "pinned": true,
        "priority": 100
      },
      "position": { "x": 500, "y": 400 },
      "expanded": true
    },
    {
      "id": "userInput-main",
      "kind": "userInput",
      "label": "Main Chat",
      "config": {},
      "position": { "x": 700, "y": 400 },
      "expanded": false
    },
    {
      "id": "tools-main",
      "kind": "tools",
      "label": "Main Tools",
      "config": {
        "tools": [
          "fs.read_file",
          "fs.read_dir",
          "fs.write_file",
          "index.search",
          "code.search_ast",
          "code.apply_edits_targeted"
        ]
      },
      "position": { "x": 700, "y": 550 },
      "expanded": false
    },
    {
      "id": "llmRequest-main",
      "kind": "llmRequest",
      "label": "Main Chat Response",
      "config": {},
      "position": { "x": 900, "y": 400 },
      "expanded": false
    },
    {
      "id": "portalInput-loop",
      "kind": "portalInput",
      "label": "Loop Back",
      "config": {
        "id": "main-loop"
      },
      "position": { "x": 1100, "y": 400 },
      "expanded": false
    },
    {
      "id": "portalOutput-loop",
      "kind": "portalOutput",
      "label": "Continue Loop",
      "config": {
        "id": "main-loop"
      },
      "position": { "x": 700, "y": 250 },
      "expanded": false
    }
  ],
  "edges": [
    {
      "id": "defaultContextStart-manualInput-bootstrap",
      "source": "defaultContextStart",
      "target": "manualInput-bootstrap",
      "sourceHandle": "context",
      "targetHandle": "context"
    },
    {
      "id": "manualInput-bootstrap-llmRequest-bootstrap-context",
      "source": "manualInput-bootstrap",
      "target": "llmRequest-bootstrap",
      "sourceHandle": "context",
      "targetHandle": "context"
    },
    {
      "id": "manualInput-bootstrap-llmRequest-bootstrap-data",
      "source": "manualInput-bootstrap",
      "target": "llmRequest-bootstrap",
      "sourceHandle": "data",
      "targetHandle": "data"
    },
    {
      "id": "tools-bootstrap-llmRequest-bootstrap",
      "source": "tools-bootstrap",
      "target": "llmRequest-bootstrap",
      "sourceHandle": "tools",
      "targetHandle": "tools"
    },
    {
      "id": "llmRequest-bootstrap-injectMessages-bootstrap-context",
      "source": "llmRequest-bootstrap",
      "target": "injectMessages-bootstrap",
      "sourceHandle": "context",
      "targetHandle": "context"
    },
    {
      "id": "llmRequest-bootstrap-injectMessages-bootstrap-assistant",
      "source": "llmRequest-bootstrap",
      "target": "injectMessages-bootstrap",
      "sourceHandle": "data",
      "targetHandle": "assistantMessage"
    },
    {
      "id": "injectMessages-bootstrap-userInput-main",
      "source": "injectMessages-bootstrap",
      "target": "userInput-main",
      "sourceHandle": "context",
      "targetHandle": "context"
    },
    {
      "id": "userInput-main-llmRequest-main-context",
      "source": "userInput-main",
      "target": "llmRequest-main",
      "sourceHandle": "context",
      "targetHandle": "context"
    },
    {
      "id": "userInput-main-llmRequest-main-data",
      "source": "userInput-main",
      "target": "llmRequest-main",
      "sourceHandle": "data",
      "targetHandle": "data"
    },
    {
      "id": "tools-main-llmRequest-main",
      "source": "tools-main",
      "target": "llmRequest-main",
      "sourceHandle": "tools",
      "targetHandle": "tools"
    },
    {
      "id": "llmRequest-main-portalInput-loop-context",
      "source": "llmRequest-main",
      "target": "portalInput-loop",
      "sourceHandle": "context",
      "targetHandle": "context"
    },
    {
      "id": "llmRequest-main-portalInput-loop-data",
      "source": "llmRequest-main",
      "target": "portalInput-loop",
      "sourceHandle": "data",
      "targetHandle": "data"
    },
    {
      "id": "portalOutput-loop-userInput-main",
      "source": "portalOutput-loop",
      "target": "userInput-main",
      "sourceHandle": "context",
      "targetHandle": "context"
    }
  ],
  "_comments": {
    "flow_description": "This flow demonstrates context bootstrapping using the injectMessages node.",
    "how_it_works": [
      "1. Flow starts with defaultContextStart node setting system instructions",
      "2. manualInput-bootstrap sends a pre-configured query asking for codebase summary",
      "3. llmRequest-bootstrap uses a low-cost LLM (gpt-4o-mini) with read-only tools to generate the summary",
      "4. injectMessages-bootstrap injects the Q&A pair into message history:",
      "   - User message comes from static config (the bootstrap question)",
      "   - Assistant message comes from dynamic input (the LLM's summary)",
      "   - Messages are pinned (pinned=true, priority=100) so they survive context windowing",
      "   - Mode is 'prepend' so they appear at the beginning of conversation history",
      "5. userInput-main receives the bootstrapped context and waits for user input",
      "6. llmRequest-main processes user queries with full context (bootstrap + conversation)",
      "7. Portal nodes create a loop back to userInput-main for multi-turn conversation"
    ],
    "idempotency": "If the flow is re-run, the injectMessages node will UPDATE the existing bootstrap messages in place rather than duplicating them. This is because each node generates stable IDs based on its node ID.",
    "context_windowing": "When context windowing is implemented, the pinned bootstrap messages (priority=100) will be kept at the top of the conversation, while unpinned messages in the middle can be removed to save tokens.",
    "customization": [
      "- Change the bootstrap query in manualInput-bootstrap config",
      "- Adjust tools in tools-bootstrap to control what the LLM can access",
      "- Modify priority in injectMessages-bootstrap (1-100, higher = more important)",
      "- Switch injectionMode to 'append' to add bootstrap at end instead of beginning"
    ]
  }
}

